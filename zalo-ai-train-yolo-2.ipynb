{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":4167.098873,"end_time":"2025-11-19T16:53:34.861253","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-19T15:44:07.762380","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9d8b5d5a","cell_type":"code","source":"!pip uninstall -y opencv-python opencv-python-headless opencv-contrib-python\n!pip install -q opencv-python==4.8.1.78\n!pip install -q ultralytics torch torchvision transformers\n!pip install -q gdown","metadata":{"execution":{"iopub.execute_input":"2025-11-19T15:44:11.125393Z","iopub.status.busy":"2025-11-19T15:44:11.124698Z","iopub.status.idle":"2025-11-19T15:45:41.723406Z","shell.execute_reply":"2025-11-19T15:45:41.722524Z"},"papermill":{"duration":90.605526,"end_time":"2025-11-19T15:45:41.725379","exception":false,"start_time":"2025-11-19T15:44:11.119853","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Found existing installation: opencv-python 4.12.0.88\r\n","Uninstalling opencv-python-4.12.0.88:\r\n","  Successfully uninstalled opencv-python-4.12.0.88\r\n","Found existing installation: opencv-python-headless 4.12.0.88\r\n","Uninstalling opencv-python-headless-4.12.0.88:\r\n","  Successfully uninstalled opencv-python-headless-4.12.0.88\r\n","Found existing installation: opencv-contrib-python 4.12.0.88\r\n","Uninstalling opencv-contrib-python-4.12.0.88:\r\n","  Successfully uninstalled opencv-contrib-python-4.12.0.88\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n","pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n","pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0m"]}],"execution_count":1},{"id":"c7650516","cell_type":"markdown","source":"## download data","metadata":{"papermill":{"duration":0.023762,"end_time":"2025-11-19T15:45:41.774416","exception":false,"start_time":"2025-11-19T15:45:41.750654","status":"completed"},"tags":[]}},{"id":"e33f7d4d","cell_type":"code","source":"!gdown --fuzzy https://drive.google.com/file/d/1kuhTWOjlNFURhCIAf6lHSIqh6l1kTHA0/view?usp=drive_link # train\n!gdown --fuzzy https://drive.google.com/file/d/19AzlsLF2u7IG23ynw6RrMg2F9jd76xNR/view?usp=drive_link # public test","metadata":{"execution":{"iopub.execute_input":"2025-11-19T15:45:41.823386Z","iopub.status.busy":"2025-11-19T15:45:41.823017Z","iopub.status.idle":"2025-11-19T15:45:58.686356Z","shell.execute_reply":"2025-11-19T15:45:58.685627Z"},"papermill":{"duration":16.889291,"end_time":"2025-11-19T15:45:58.687769","exception":false,"start_time":"2025-11-19T15:45:41.798478","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading...\r\n","From (original): https://drive.google.com/uc?id=1kuhTWOjlNFURhCIAf6lHSIqh6l1kTHA0\r\n","From (redirected): https://drive.google.com/uc?id=1kuhTWOjlNFURhCIAf6lHSIqh6l1kTHA0&confirm=t&uuid=9a0c3cd2-26be-42e9-ae55-0a9104d0e75b\r\n","To: /kaggle/working/train.zip\r\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 636M/636M [00:06<00:00, 99.8MB/s]\r\n","Downloading...\r\n","From (original): https://drive.google.com/uc?id=19AzlsLF2u7IG23ynw6RrMg2F9jd76xNR\r\n","From (redirected): https://drive.google.com/uc?id=19AzlsLF2u7IG23ynw6RrMg2F9jd76xNR&confirm=t&uuid=2b014a75-dba6-432b-ad71-cada0faa013b\r\n","To: /kaggle/working/public_test.zip\r\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 281M/281M [00:02<00:00, 107MB/s]\r\n"]}],"execution_count":2},{"id":"31e30e19","cell_type":"code","source":"%%capture\n!unzip train.zip\n!unzip public_test.zip","metadata":{"execution":{"iopub.execute_input":"2025-11-19T15:45:58.743342Z","iopub.status.busy":"2025-11-19T15:45:58.742626Z","iopub.status.idle":"2025-11-19T15:46:04.872523Z","shell.execute_reply":"2025-11-19T15:46:04.871479Z"},"papermill":{"duration":6.158629,"end_time":"2025-11-19T15:46:04.873933","exception":false,"start_time":"2025-11-19T15:45:58.715304","status":"completed"},"tags":[]},"outputs":[],"execution_count":3},{"id":"fbe21ee6","cell_type":"markdown","source":"## setting","metadata":{"papermill":{"duration":0.026622,"end_time":"2025-11-19T15:46:04.927899","exception":false,"start_time":"2025-11-19T15:46:04.901277","status":"completed"},"tags":[]}},{"id":"93052de1","cell_type":"code","source":"import os\nimport json\nimport cv2\nimport yaml\nimport random\nimport torch\nimport shutil\nfrom pathlib import Path\nimport numpy as np\nimport re\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport glob\nfrom ultralytics import YOLO","metadata":{"execution":{"iopub.execute_input":"2025-11-19T15:46:04.982776Z","iopub.status.busy":"2025-11-19T15:46:04.982058Z","iopub.status.idle":"2025-11-19T15:46:07.787616Z","shell.execute_reply":"2025-11-19T15:46:07.786995Z"},"papermill":{"duration":2.834192,"end_time":"2025-11-19T15:46:07.788758","exception":false,"start_time":"2025-11-19T15:46:04.954566","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating new Ultralytics Settings v0.0.6 file âœ… \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"execution_count":4},{"id":"2a910ef7","cell_type":"code","source":"# Config\nDATASET_ROOT = \"train\"\nTEST_ROOT = \"public_test/samples\"\nANNOTATIONS_PATH = os.path.join(DATASET_ROOT, \"annotations/annotations.json\")\nSAMPLES_DIR = os.path.join(DATASET_ROOT, \"samples\")\nWORK_DIR = \"/kaggle/working/stream_dataset\"\nos.makedirs(WORK_DIR, exist_ok=True)\nOUTPUT_PATH = \"/kaggle/working/submission.json\"\n\nTRAIN_RATIO = 0.85\nEPOCHS = 50\nBATCH = 32\nNUM_WORKERS = 4\nYOLO_MODEL = \"yolov8l.pt\"\nCLASS_NAMES = [\"target\"]","metadata":{"execution":{"iopub.execute_input":"2025-11-19T15:46:07.843326Z","iopub.status.busy":"2025-11-19T15:46:07.842976Z","iopub.status.idle":"2025-11-19T15:46:07.847714Z","shell.execute_reply":"2025-11-19T15:46:07.846931Z"},"papermill":{"duration":0.032902,"end_time":"2025-11-19T15:46:07.848779","exception":false,"start_time":"2025-11-19T15:46:07.815877","status":"completed"},"tags":[]},"outputs":[],"execution_count":5},{"id":"c653a9b5","cell_type":"code","source":"# model = YOLO(TRAINED_MODEL_PATH)\n\n# model.info()","metadata":{"execution":{"iopub.execute_input":"2025-11-19T15:46:07.903087Z","iopub.status.busy":"2025-11-19T15:46:07.902509Z","iopub.status.idle":"2025-11-19T15:46:07.905568Z","shell.execute_reply":"2025-11-19T15:46:07.904975Z"},"papermill":{"duration":0.03164,"end_time":"2025-11-19T15:46:07.906598","exception":false,"start_time":"2025-11-19T15:46:07.874958","status":"completed"},"tags":[]},"outputs":[],"execution_count":6},{"id":"f59b01ff","cell_type":"code","source":"# Load annotations\nwith open(ANNOTATIONS_PATH, \"r\") as f:\n    annotations = json.load(f)\n\nvideo_ids = [a[\"video_id\"] for a in annotations]\nann_dict = {a[\"video_id\"]: a for a in annotations}","metadata":{"execution":{"iopub.execute_input":"2025-11-19T15:46:07.960149Z","iopub.status.busy":"2025-11-19T15:46:07.959926Z","iopub.status.idle":"2025-11-19T15:46:07.994067Z","shell.execute_reply":"2025-11-19T15:46:07.993542Z"},"papermill":{"duration":0.062275,"end_time":"2025-11-19T15:46:07.995193","exception":false,"start_time":"2025-11-19T15:46:07.932918","status":"completed"},"tags":[]},"outputs":[],"execution_count":7},{"id":"524549ec","cell_type":"code","source":"# Split train/val\nrandom.seed(42)\nrandom.shuffle(video_ids)\nsplit_idx = int(len(video_ids) * TRAIN_RATIO)\ntrain_videos = video_ids[:split_idx]\nval_videos = video_ids[split_idx:]\nprint(f\"Loaded {len(video_ids)} videos â€” Train: {len(train_videos)}, Val: {len(val_videos)}\")","metadata":{"execution":{"iopub.execute_input":"2025-11-19T15:46:08.049644Z","iopub.status.busy":"2025-11-19T15:46:08.049418Z","iopub.status.idle":"2025-11-19T15:46:08.053764Z","shell.execute_reply":"2025-11-19T15:46:08.053224Z"},"papermill":{"duration":0.03266,"end_time":"2025-11-19T15:46:08.054699","exception":false,"start_time":"2025-11-19T15:46:08.022039","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 14 videos â€” Train: 11, Val: 3\n"]}],"execution_count":8},{"id":"8f9185f4","cell_type":"code","source":"# Prepare directories\ndef prepare_dirs(base_dir):\n    imgs = os.path.join(base_dir, \"images\")\n    lbls = os.path.join(base_dir, \"labels\")\n    os.makedirs(imgs, exist_ok=True)\n    os.makedirs(lbls, exist_ok=True)\n    return imgs, lbls\n\ntrain_img_dir, train_lbl_dir = prepare_dirs(os.path.join(WORK_DIR, \"train\"))\nval_img_dir, val_lbl_dir = prepare_dirs(os.path.join(WORK_DIR, \"val\"))","metadata":{"execution":{"iopub.execute_input":"2025-11-19T15:46:08.109071Z","iopub.status.busy":"2025-11-19T15:46:08.108567Z","iopub.status.idle":"2025-11-19T15:46:08.113105Z","shell.execute_reply":"2025-11-19T15:46:08.112601Z"},"papermill":{"duration":0.033016,"end_time":"2025-11-19T15:46:08.114151","exception":false,"start_time":"2025-11-19T15:46:08.081135","status":"completed"},"tags":[]},"outputs":[],"execution_count":9},{"id":"78950836","cell_type":"code","source":"EMPTY_FRAME_STEP = 30 \n\ndef convert_to_yolo_format(x1, y1, x2, y2, img_width, img_height, class_id):\n    dw = 1.0 / img_width\n    dh = 1.0 / img_height\n    x_center = (x1 + x2) / 2.0\n    y_center = (y1 + y2) / 2.0\n    width = x2 - x1\n    height = y2 - y1\n    \n    x_center = max(0, min(1, x_center * dw))\n    y_center = max(0, min(1, y_center * dh))\n    width = max(0, min(1, width * dw))\n    height = max(0, min(1, height * dh))\n    \n    return f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n\ndef extract_frames_and_labels(video_id, mode=\"train\"):\n    video_dir = os.path.join(SAMPLES_DIR, video_id)\n    video_path = os.path.join(video_dir, \"drone_video.mp4\")\n    \n    if not os.path.exists(video_path):\n        return f\"Missing video: {video_path}\"\n\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        return f\"Cannot open: {video_path}\"\n\n    img_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    img_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    frame_annotations = {}\n    video_record = ann_dict.get(video_id, {})\n    \n    for interval in video_record.get(\"annotations\", []):\n        for bbox_item in interval.get(\"bboxes\", []):\n            frame_num = bbox_item['frame']\n            if frame_num not in frame_annotations:\n                frame_annotations[frame_num] = []\n            frame_annotations[frame_num].append(bbox_item)\n\n    img_out = train_img_dir if mode == \"train\" else val_img_dir\n    lbl_out = train_lbl_dir if mode == \"train\" else val_lbl_dir\n    \n    saved_obj = 0\n    saved_bg = 0\n\n    frame_idx = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        should_save = False\n        is_background = False\n\n        if frame_idx in frame_annotations:\n            should_save = True\n            is_background = False\n        \n        elif frame_idx % EMPTY_FRAME_STEP == 0:\n            should_save = True\n            is_background = True\n        \n        if should_save:\n            image_name_stem = f\"{video_id}_frame_{frame_idx:06d}\"\n            img_path = os.path.join(img_out, image_name_stem + \".jpg\")\n            txt_path = os.path.join(lbl_out, image_name_stem + \".txt\")\n\n            success = cv2.imwrite(img_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 95])\n            if not success:\n                print(f\"Failed to write: {img_path}\")\n                frame_idx += 1\n                continue\n\n            # LÆ°u Label\n            if not is_background:\n                yolo_labels = []\n                for bbox in frame_annotations[frame_idx]:\n                    yolo_line = convert_to_yolo_format(\n                        bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2'], \n                        img_width, img_height, 0\n                    )\n                    yolo_labels.append(yolo_line)\n                \n                with open(txt_path, \"w\") as f:\n                    f.write('\\n'.join(yolo_labels))\n                saved_obj += 1\n            else:\n                # Ghi file rá»—ng (Background image)\n                with open(txt_path, \"w\") as f:\n                    pass \n                saved_bg += 1\n        \n        frame_idx += 1\n\n    cap.release()\n    return f\"{video_id}: {saved_obj} annotated + {saved_bg} background frames saved.\"\n\nprint(\"Extracting frames + labels ...\")\nfutures = []\nwith ThreadPoolExecutor(max_workers=NUM_WORKERS) as ex:\n    for vid in train_videos:\n        futures.append(ex.submit(extract_frames_and_labels, vid, \"train\"))\n    for vid in val_videos:\n        futures.append(ex.submit(extract_frames_and_labels, vid, \"val\"))\n    \n    for i, fut in enumerate(as_completed(futures), 1):\n        print(f\"[{i}/{len(futures)}] {fut.result()}\")\n\nprint(\"All videos processed!\")\n\nprint(\"\\nValidating extracted images...\")\ntrain_imgs = glob.glob(os.path.join(train_img_dir, \"*.jpg\"))\nval_imgs = glob.glob(os.path.join(val_img_dir, \"*.jpg\"))\nprint(f\"  Train images: {len(train_imgs)}\")\nprint(f\"  Val images: {len(val_imgs)}\")\n\ntest_success = 0\nfor img_path in train_imgs[:5]:\n    img = cv2.imread(img_path)\n    if img is not None:\n        test_success += 1\nprint(f\" Test read: {test_success}/5 successful \")\n\ndata_yaml = {\n    \"train\": os.path.abspath(train_img_dir),\n    \"val\": os.path.abspath(val_img_dir),\n    \"nc\": 1,                 \n    \"names\": CLASS_NAMES\n}\ndata_path = os.path.join(WORK_DIR, \"data.yaml\")\nwith open(data_path, \"w\") as f:\n    yaml.dump(data_yaml, f)\nprint(f\"data.yaml created for Single Class\")","metadata":{"execution":{"iopub.execute_input":"2025-11-19T15:46:08.168275Z","iopub.status.busy":"2025-11-19T15:46:08.168045Z","iopub.status.idle":"2025-11-19T15:49:54.428503Z","shell.execute_reply":"2025-11-19T15:49:54.427630Z"},"papermill":{"duration":226.316218,"end_time":"2025-11-19T15:49:54.456883","exception":false,"start_time":"2025-11-19T15:46:08.140665","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Extracting frames + labels ...\n","[1/14] WaterBottle_0: 934 annotated + 136 background frames saved.\n","[2/14] Lifering_0: 1134 annotated + 120 background frames saved.\n","[3/14] MobilePhone_0: 968 annotated + 181 background frames saved.\n","[4/14] Lifering_1: 1511 annotated + 174 background frames saved.\n","[5/14] Person1_1: 1129 annotated + 134 background frames saved.\n","[6/14] Laptop_1: 987 annotated + 139 background frames saved.\n","[7/14] Jacket_0: 1162 annotated + 133 background frames saved.\n","[8/14] MobilePhone_1: 889 annotated + 134 background frames saved.\n","[9/14] Jacket_1: 690 annotated + 152 background frames saved.\n","[10/14] WaterBottle_1: 3123 annotated + 122 background frames saved.\n","[11/14] Laptop_0: 884 annotated + 145 background frames saved.\n","[12/14] Backpack_1: 1454 annotated + 102 background frames saved.\n","[13/14] Person1_0: 2057 annotated + 118 background frames saved.\n","[14/14] Backpack_0: 3184 annotated + 243 background frames saved.\n","All videos processed!\n","\n","Validating extracted images...\n","  Train images: 14981\n","  Val images: 7158\n"," Test read: 5/5 successful \n","data.yaml created for Single Class\n"]}],"execution_count":10},{"id":"401eeb8c","cell_type":"code","source":"model = YOLO(YOLO_MODEL)","metadata":{"execution":{"iopub.execute_input":"2025-11-19T15:49:54.513866Z","iopub.status.busy":"2025-11-19T15:49:54.513237Z","iopub.status.idle":"2025-11-19T15:49:57.374475Z","shell.execute_reply":"2025-11-19T15:49:57.373817Z"},"papermill":{"duration":2.891933,"end_time":"2025-11-19T15:49:57.375764","exception":false,"start_time":"2025-11-19T15:49:54.483831","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8l.pt to 'yolov8l.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 83.7MB 41.7MB/s 2.0s\n"]}],"execution_count":11},{"id":"cc09b6b0","cell_type":"code","source":"# Train model\nprint(\"\\nStarting YOLO training...\")\nresults = model.train(\n    data=data_path,\n    epochs=EPOCHS,\n    batch=BATCH,\n    imgsz=640,\n    workers=NUM_WORKERS,          \n    exist_ok=True,      \n    project=\"/kaggle/working/yolo_finetune\",\n    device=[0, 1],  # ThÃªm dÃ²ng nÃ y Ä‘á»ƒ sá»­ dá»¥ng GPU 0 vÃ  1\n)\nprint(\"Training completed!\")","metadata":{"execution":{"iopub.execute_input":"2025-11-19T15:49:57.432697Z","iopub.status.busy":"2025-11-19T15:49:57.431990Z","iopub.status.idle":"2025-11-19T16:36:32.127617Z","shell.execute_reply":"2025-11-19T16:36:32.126688Z"},"papermill":{"duration":2794.72528,"end_time":"2025-11-19T16:36:32.129294","exception":false,"start_time":"2025-11-19T15:49:57.404014","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Starting YOLO training...\n","Ultralytics 8.3.229 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n","                                                       CUDA:1 (Tesla T4, 15095MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/working/stream_dataset/data.yaml, degrees=0.0, deterministic=True, device=0,1, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=5, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/kaggle/working/yolo_finetune, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/kaggle/working/yolo_finetune/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n","\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 4.8MB/s 0.2s\n","Overriding model.yaml nc=80 with nc=1\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n","  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n","  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n","  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n","  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n","  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n","  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n","  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n"," 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n"," 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n"," 22        [15, 18, 21]  1   5583571  ultralytics.nn.modules.head.Detect           [1, [256, 512, 512]]          \n","Model summary: 209 layers, 43,630,611 parameters, 43,630,595 gradients, 165.4 GFLOPs\n","\n","Transferred 589/595 items from pretrained weights\n","\u001b[34m\u001b[1mDDP:\u001b[0m debug command /usr/bin/python3 -m torch.distributed.run --nproc_per_node 2 --master_port 45867 /root/.config/Ultralytics/DDP/_temp_xtht_s63133089980679568.py\n","Ultralytics 8.3.229 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n","                                                       CUDA:1 (Tesla T4, 15095MiB)\n","Overriding model.yaml nc=80 with nc=1\n","Transferred 589/595 items from pretrained weights\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 19.6MB/s 0.3s\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2736.7Â±429.4 MB/s, size: 252.5 KB)\n","\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/stream_dataset/train/labels... 14981 images, 1570 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14981/14981 1.6Kit/s 9.4s\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Jacket_0_frame_001719.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Jacket_0_frame_002855.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Jacket_0_frame_003789.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Jacket_1_frame_001650.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Jacket_1_frame_002623.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Jacket_1_frame_005075.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Laptop_0_frame_002916.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Laptop_0_frame_003084.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Laptop_0_frame_003171.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Laptop_0_frame_003642.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Laptop_0_frame_004825.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Laptop_1_frame_002230.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Laptop_1_frame_004759.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Lifering_0_frame_002605.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Lifering_0_frame_002984.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Lifering_1_frame_000105.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Lifering_1_frame_003060.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Lifering_1_frame_003338.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Lifering_1_frame_005279.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Lifering_1_frame_006070.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/MobilePhone_0_frame_000587.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/MobilePhone_0_frame_004607.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/MobilePhone_0_frame_004840.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/MobilePhone_0_frame_005738.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/MobilePhone_1_frame_001063.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/MobilePhone_1_frame_002462.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/MobilePhone_1_frame_004125.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/MobilePhone_1_frame_004797.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Person1_1_frame_002738.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Person1_1_frame_002951.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Person1_1_frame_003382.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Person1_1_frame_004099.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/Person1_1_frame_004628.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_0_frame_003670.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_0_frame_003859.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_0_frame_004299.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_0_frame_004921.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_1_frame_001043.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_1_frame_001975.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_1_frame_002044.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_1_frame_002052.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_1_frame_002053.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_1_frame_002057.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_1_frame_002064.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_1_frame_002075.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_1_frame_004731.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_1_frame_005627.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_1_frame_005828.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/stream_dataset/train/images/WaterBottle_1_frame_006556.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/stream_dataset/train/labels.cache\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1180.3Â±1165.3 MB/s, size: 285.4 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/stream_dataset/val/labels... 7158 images, 463 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7158/7158 1.4Kit/s 5.0s\n","\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/stream_dataset/val/images/Backpack_0_frame_003681.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/stream_dataset/val/images/Backpack_0_frame_005107.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/stream_dataset/val/images/Backpack_0_frame_005565.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/stream_dataset/val/images/Backpack_0_frame_005931.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/stream_dataset/val/images/Backpack_0_frame_006302.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/stream_dataset/val/images/Backpack_1_frame_003624.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/stream_dataset/val/images/Backpack_1_frame_004127.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/stream_dataset/val/images/Person1_0_frame_002704.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/stream_dataset/val/images/Person1_0_frame_003020.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/stream_dataset/val/images/Person1_0_frame_003263.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/stream_dataset/val/images/Person1_0_frame_003474.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/stream_dataset/val/images/Person1_0_frame_003671.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/stream_dataset/val/images/Person1_0_frame_004534.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/stream_dataset/val/images/Person1_0_frame_004763.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/stream_dataset/val/labels.cache\n","Plotting labels to /kaggle/working/yolo_finetune/train/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 4 dataloader workers\n","Logging results to \u001b[1m/kaggle/working/yolo_finetune/train\u001b[0m\n","Starting training for 50 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       1/50      9.39G      1.041      1.019     0.9191          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 469/469 1.2it/s 6:36\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 2.2it/s 51.6s\n","                   all       7158       6695      0.855      0.618      0.713      0.405\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       2/50      9.82G      1.058     0.5626      0.919          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 469/469 1.2it/s 6:42\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 2.2it/s 50.1s\n","                   all       7158       6695      0.796      0.412      0.531      0.253\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       3/50       9.7G      1.094     0.5999     0.9251          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 469/469 1.2it/s 6:37\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 2.2it/s 50.7s\n","                   all       7158       6695      0.674      0.487      0.503      0.245\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       4/50      9.87G      1.092     0.6018     0.9285          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 469/469 1.2it/s 6:36\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 2.2it/s 50.6s\n","                   all       7158       6695      0.361      0.143      0.122     0.0456\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       5/50       9.7G      1.032     0.5594     0.9163          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 469/469 1.2it/s 6:35\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 2.2it/s 50.0s\n","                   all       7158       6695       0.45      0.168      0.144     0.0613\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K       6/50      9.91G     0.9787     0.5227     0.9052          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 469/469 1.2it/s 6:35\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 2.3it/s 48.1s\n","                   all       7158       6695      0.794      0.221      0.254      0.116\n","\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 5 epochs. Best results observed at epoch 1, best model saved as best.pt.\n","To update EarlyStopping(patience=5) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n","\n","6 epochs completed in 0.749 hours.\n","Optimizer stripped from /kaggle/working/yolo_finetune/train/weights/last.pt, 87.6MB\n","Optimizer stripped from /kaggle/working/yolo_finetune/train/weights/best.pt, 87.6MB\n","\n","Validating /kaggle/working/yolo_finetune/train/weights/best.pt...\n","Model summary (fused): 112 layers, 43,607,379 parameters, 0 gradients, 164.8 GFLOPs\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 2.2it/s 50.5s\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n","  xa[xa < 0] = -1\n","/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n","  xa[xa < 0] = -1\n"]},{"name":"stdout","output_type":"stream","text":["                   all       7158       6695      0.855      0.618      0.713      0.405\n","Speed: 0.1ms preprocess, 5.0ms inference, 0.0ms loss, 0.6ms postprocess per image\n","Results saved to \u001b[1m/kaggle/working/yolo_finetune/train\u001b[0m\n","Training completed!\n"]}],"execution_count":12},{"id":"3dde3162","cell_type":"code","source":"class YOLODetector:\n    def __init__(self, model_path, confidence=0.0001):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model = YOLO(model_path)\n        # self.model.set_classes(['target'])\n        self.conf = confidence\n\n    def detect_video(self, video_path):\n        \"\"\"\n        Cháº¡y model YOLO trÃªn video vÃ  tráº£ vá» táº¥t cáº£ cÃ¡c detection tÃ¬m tháº¥y.\n        \"\"\"\n        if not os.path.exists(video_path):\n            print(f\"Warning: File {video_path} not found.\")\n            return []\n\n        detections = []\n\n        results = self.model.predict(\n            source=video_path,\n            conf=self.conf,\n            iou=0.5,        # NMS IoU threshold cá»§a YOLO\n            imgsz=640,\n            stream=True,\n            verbose=False\n        )\n\n        for frame_idx, r in enumerate(results):\n            if r.boxes is not None and len(r.boxes) > 0:\n                # Láº¥y tá»a Ä‘á»™ xyxy vÃ  chuyá»ƒn sang numpy\n                boxes = r.boxes.xyxy.cpu().numpy()\n                \n                # LÆ°u láº¡i táº¥t cáº£ cÃ¡c box tÃ¬m tháº¥y trong frame nÃ y\n                for (x1, y1, x2, y2) in boxes:\n                    detections.append({\n                        \"frame\": frame_idx,\n                        \"x1\": int(x1), \n                        \"y1\": int(y1),\n                        \"x2\": int(x2), \n                        \"y2\": int(y2)\n                    })\n\n        print(f\"-> Found {len(detections)} raw detections.\")\n        return detections\n\n    def process_test_set(self, test_root, output_json):\n        \"\"\"\n        Duyá»‡t qua thÆ° má»¥c test, cháº¡y detect vÃ  lÆ°u vÃ o JSON.\n        \"\"\"\n        video_dirs = sorted([d for d in Path(test_root).iterdir() if d.is_dir()])\n        submission = []\n        \n        print(f\"Starting processing on {self.device}...\")\n        \n        for i, vid_dir in enumerate(video_dirs, 1):\n            video_path = vid_dir / \"drone_video.mp4\"\n            print(f\"\\n[{i}/{len(video_dirs)}] ğŸ¥ {vid_dir.name}\")\n\n            if 'images' in vid_dir.name:\n                continue\n                            \n            # Láº¥y danh sÃ¡ch raw detections\n            detections = self.detect_video(str(video_path))\n\n            # ÄÃ³ng gÃ³i theo format yÃªu cáº§u\n            submission.append({\n                \"video_id\": vid_dir.name,\n                \"detections\": [{\"bboxes\": detections}] if detections else []\n            })\n\n        # LÆ°u file JSON\n        with open(output_json, \"w\") as f:\n            json.dump(submission, f, indent=2)\n\n        print(f\"\\nâœ… Process completed. Saved to: {output_json}\")\n        return submission","metadata":{"execution":{"iopub.execute_input":"2025-11-19T16:36:32.483166Z","iopub.status.busy":"2025-11-19T16:36:32.482882Z","iopub.status.idle":"2025-11-19T16:36:32.492716Z","shell.execute_reply":"2025-11-19T16:36:32.492152Z"},"papermill":{"duration":0.186385,"end_time":"2025-11-19T16:36:32.494007","exception":false,"start_time":"2025-11-19T16:36:32.307622","status":"completed"},"tags":[]},"outputs":[],"execution_count":13},{"id":"efd55878","cell_type":"code","source":"# Run inference\nTRAINED_MODEL_PATH = 'yolo_finetune/train/weights/best.pt' \ndetector = YOLODetector(TRAINED_MODEL_PATH, confidence=0.05)\n\nsubmission = detector.process_test_set(\n    test_root= TEST_ROOT,\n    output_json=\"/kaggle/working/submission.json\"\n)\nprint(\"COMPLETE\")","metadata":{"execution":{"iopub.execute_input":"2025-11-19T16:36:32.848247Z","iopub.status.busy":"2025-11-19T16:36:32.847958Z","iopub.status.idle":"2025-11-19T16:53:32.837707Z","shell.execute_reply":"2025-11-19T16:53:32.836787Z"},"papermill":{"duration":1020.440542,"end_time":"2025-11-19T16:53:33.109191","exception":false,"start_time":"2025-11-19T16:36:32.668649","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting processing on cuda...\n","\n","[1/12] ğŸ¥ BlackBox_0\n","-> Found 2072 raw detections.\n","\n","[2/12] ğŸ¥ BlackBox_0_images\n","Warning: File public_test/samples/BlackBox_0_images/drone_video.mp4 not found.\n","\n","[3/12] ğŸ¥ BlackBox_1\n","-> Found 1284 raw detections.\n","\n","[4/12] ğŸ¥ BlackBox_1_images\n","Warning: File public_test/samples/BlackBox_1_images/drone_video.mp4 not found.\n","\n","[5/12] ğŸ¥ CardboardBox_0\n","-> Found 425 raw detections.\n","\n","[6/12] ğŸ¥ CardboardBox_0_images\n","Warning: File public_test/samples/CardboardBox_0_images/drone_video.mp4 not found.\n","\n","[7/12] ğŸ¥ CardboardBox_1\n","-> Found 314 raw detections.\n","\n","[8/12] ğŸ¥ CardboardBox_1_images\n","Warning: File public_test/samples/CardboardBox_1_images/drone_video.mp4 not found.\n","\n","[9/12] ğŸ¥ LifeJacket_0\n","-> Found 7746 raw detections.\n","\n","[10/12] ğŸ¥ LifeJacket_0_images\n","Warning: File public_test/samples/LifeJacket_0_images/drone_video.mp4 not found.\n","\n","[11/12] ğŸ¥ LifeJacket_1\n","-> Found 4404 raw detections.\n","\n","[12/12] ğŸ¥ LifeJacket_1_images\n","Warning: File public_test/samples/LifeJacket_1_images/drone_video.mp4 not found.\n","\n","âœ… Process completed. Saved to: /kaggle/working/submission.json\n","COMPLETE\n"]}],"execution_count":14},{"id":"420bf9d3","cell_type":"code","source":"","metadata":{"papermill":{"duration":0.178663,"end_time":"2025-11-19T16:53:33.465453","exception":false,"start_time":"2025-11-19T16:53:33.286790","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}